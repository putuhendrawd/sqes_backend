{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;sys.path.append('..')\n",
    "from sqes_function import Config, DBPool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read_inventory\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.dialects import postgresql\n",
    "from urllib.parse import quote\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pandas only supports SQLAlchemy connectable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data source\n",
    "\n",
    "url = f'http://202.90.198.40/sismon-wrs/assets/sismon-slmon2/data/slmon.all.laststatus.json'\n",
    "html = requests.get(url).content\n",
    "json_data = json.loads(html)\n",
    "json_data = json_data['features']\n",
    "\n",
    "# list sta in json_data \n",
    "stations = []\n",
    "for item in json_data:\n",
    "    if 'properties' in item and 'sta' in item['properties']:\n",
    "        stations.append(item['properties']['sta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open db connection and read stations\n",
    "db_config = Config.load_config(section=\"postgresql\") \n",
    "db_pool = DBPool(**db_config) # type: ignore\n",
    "encoded_password = quote(db_config['password'])\n",
    "engine2 = create_engine(f\"postgresql+psycopg2://{db_config['user']}:{encoded_password}@{db_config['host']}:{db_config['port']}/{db_config['database']}\")\n",
    "\n",
    "# read stations from db\n",
    "stations_db = pd.read_sql('select code,latitude,longitude from stations', con=engine2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPDATE STATION METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see difference between json and db station\n",
    "json_not_in_db = list(set(stations) - set(stations_db['code'].tolist()))\n",
    "print(f\"Stations in JSON not in DB: {json_not_in_db}\")\n",
    "db_not_in_json = list(set(stations_db['code'].tolist()) - set(stations))\n",
    "print(f\"Stations in DB not in JSON: {db_not_in_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert new stations from json to db\n",
    "if json_not_in_db:\n",
    "    for sta in json_not_in_db:\n",
    "        # find station in json_data\n",
    "        station_data = next((item for item in json_data if item['properties']['sta'] == sta), None)\n",
    "        if station_data:\n",
    "            longitude = station_data['geometry'].get('coordinates', None)[0]\n",
    "            latitude = station_data['geometry'].get('coordinates', None)[1]\n",
    "            network = station_data['properties'].get('net', None)\n",
    "            province = station_data['properties'].get('provin', None)\n",
    "            location = station_data['properties'].get('location', None)\n",
    "            year = None # no year data available in json\n",
    "            upt = station_data['properties'].get('uptbmkg', None)\n",
    "            balai = None\n",
    "            digitizer_type = station_data['properties'].get('merkdgtz', None)\n",
    "            match = re.search(r'(?:19|20)\\d{2}-(.*)', digitizer_type) if digitizer_type else None\n",
    "            communication_type = match.group(1) if match else None\n",
    "            network_group = None\n",
    "            \n",
    "            if latitude is not None and longitude is not None:\n",
    "                print(f\"Inserting {sta} with lat: {latitude}, lon: {longitude}, network: {network}, province: {province}, location: {location}, year: {year}, upt: {upt}, balai: {balai}, digitizer_type: {digitizer_type}, communication_type: {communication_type}\")\n",
    "                # insert into db\n",
    "                db_pool.execute(\n",
    "                    \"INSERT INTO stations (code, network, latitude, longitude, province, location, year, upt, balai, digitizer_type, communication_type, network_group) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\",\n",
    "                    (sta, network, latitude, longitude, province, location, year, upt, balai, digitizer_type, communication_type, network_group),\n",
    "                    commit=True\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Skipping {sta}, missing latitude or longitude.\")\n",
    "        else:\n",
    "            print(f\"Station {sta} not found in JSON data.\")\n",
    "    # update stations_db after insertion\n",
    "    stations_db = pd.read_sql('select code,latitude,longitude from stations', con=engine2)\n",
    "else:\n",
    "    print(\"No new stations to insert from JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function see diff df1 that not in df2\n",
    "def see_diff(df1, df2):\n",
    "    diff_list = []\n",
    "    if len(df1) != len(df2):\n",
    "        print(\"differences:\")\n",
    "        df1_not_in_df2 = list(set(df1['code'].tolist()) - set(df2['code'].tolist()))\n",
    "        for sta in df1_not_in_df2:\n",
    "            if sta not in df2.code.tolist():\n",
    "                diff_list.append(sta)\n",
    "        print(diff_list)\n",
    "\n",
    "# Sync stations data to other tables\n",
    "stations_db = pd.read_sql('select code from stations', con=engine2)\n",
    "stations_dominant_data_quality_db = pd.read_sql('select code from stations_dominant_data_quality', con=engine2)\n",
    "stations_site_quality_db = pd.read_sql('select code from stations_site_quality', con=engine2)\n",
    "stations_visit_db = pd.read_sql('select code from stations_visit', con=engine2)\n",
    "\n",
    "print(f\"** Stations in stations: {len(stations_db)} ; unique : {len(stations_db['code'].unique())}\")\n",
    "print(f\"\\n** Stations in stations_dominant_data_quality: {len(stations_dominant_data_quality_db)} ; unique : {len(stations_dominant_data_quality_db['code'].unique())}\")\n",
    "stations_not_in_dominant = list(set(stations_db['code'].tolist()) - set(stations_dominant_data_quality_db['code'].tolist()))\n",
    "see_diff(stations_dominant_data_quality_db, stations_db)\n",
    "# see_diff(stations_db, stations_dominant_data_quality_db)\n",
    "print(\"stations_dominant_data_quality not in stations\",len(stations_not_in_dominant),stations_not_in_dominant)\n",
    "\n",
    "print(f\"\\n** Stations in stations_site_quality: {len(stations_site_quality_db)} ; unique : {len(stations_site_quality_db['code'].unique())}\")\n",
    "stations_not_in_site_quality = list(set(stations_db['code'].tolist()) - set(stations_site_quality_db['code'].tolist()))\n",
    "see_diff(stations_site_quality_db, stations_db)\n",
    "# see_diff(stations_db,stations_site_quality_db)\n",
    "print(\"stations_site_quality not in stations\",len(stations_not_in_site_quality),stations_not_in_site_quality)\n",
    "\n",
    "print(f\"\\n** Stations in stations_visit: {len(stations_visit_db)} ; unique : {len(stations_visit_db['code'].unique())}\")\n",
    "stations_not_in_visit = list(set(stations_db['code'].tolist()) - set(stations_visit_db['code'].tolist()))\n",
    "see_diff(stations_visit_db, stations_db)\n",
    "# see_diff(stations_db, stations_visit_db)\n",
    "print(\"stations_visit not in stations\",len(stations_not_in_visit), stations_not_in_visit)\n",
    "\n",
    "\n",
    "# Sync Process\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "# Sync stations_dominant_data_quality\n",
    "if stations_not_in_dominant:\n",
    "    for sta in stations_not_in_dominant:\n",
    "        print(f\"Inserting {sta} into stations_dominant_data_quality\")\n",
    "        db_pool.execute(\n",
    "            \"INSERT INTO stations_dominant_data_quality (code) VALUES (%s)\",\n",
    "            (sta,),\n",
    "            commit=True\n",
    "        )\n",
    "print(\"Done inserting stations_dominant_data_quality\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "# Sync stations_site_quality\n",
    "if stations_not_in_site_quality:\n",
    "    for sta in stations_not_in_site_quality:\n",
    "        print(f\"Inserting {sta} into stations_site_quality\")\n",
    "        db_pool.execute(\n",
    "            \"INSERT INTO stations_site_quality (code) VALUES (%s)\",\n",
    "            (sta,),\n",
    "            commit=True\n",
    "        )\n",
    "print(\"Done inserting stations_site_quality\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "# Sync stations_visit\n",
    "if stations_not_in_visit:\n",
    "    for sta in stations_not_in_visit:\n",
    "        print(f\"Inserting {sta} into stations_visit\")\n",
    "        db_pool.execute(\n",
    "            \"INSERT INTO stations_visit (code) VALUES (%s)\",\n",
    "            (sta,),\n",
    "            commit=True\n",
    "        )\n",
    "print(\"Done inserting stations_visit\")\n",
    "print(\"-----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data from xml\n",
    "rows_list = []\n",
    "loop_obj = tqdm(stations_db['code'].tolist())\n",
    "for code in loop_obj:\n",
    "    try:\n",
    "        loop_obj.set_description(\"Processing %s\" % code)\n",
    "        inv = read_inventory(f\"https://geof.bmkg.go.id/fdsnws/station/1/query?station={code}&level=response&nodata=404\")\n",
    "        dict_xml = {\n",
    "        'code': inv[0].stations[0].code,\n",
    "        'latitude_xml': inv[0].stations[0].latitude,\n",
    "        'longitude_xml' : inv[0].stations[0].longitude\n",
    "        }\n",
    "        rows_list.append(dict_xml)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing station {code}: {e}\")\n",
    "stations_xml = pd.DataFrame(rows_list)\n",
    "\n",
    "# merge\n",
    "merge = stations_db.join(stations_xml.set_index('code'), on='code')\n",
    "merge['latitude_diff'] = abs(merge.latitude - merge.latitude_xml)\n",
    "merge['longitude_diff'] = abs(merge.latitude - merge.latitude_xml)\n",
    "\n",
    "# check station lat and lon difference based on threshold\n",
    "threshold = 0 #1/111\n",
    "merge[(merge.latitude_diff > threshold) | (merge.longitude_diff > threshold)]\n",
    "\n",
    "# update db\n",
    "print(\"update stations latitude and longitude based on xml data\")\n",
    "sql=\"\"\"UPDATE stations SET latitude=%s, longitude=%s WHERE code=%s\"\"\"\n",
    "rowcount=0\n",
    "\n",
    "for data in merge[(merge.latitude_diff > threshold) | (merge.longitude_diff > threshold)].iterrows():\n",
    "    print(\"Processing %s\" % data[1].code)\n",
    "    # take data from station xml\n",
    "    dict_xml = {\n",
    "    'code': data[1].code,\n",
    "    'latitude': data[1].latitude_xml,\n",
    "    'longitude' : data[1].longitude_xml\n",
    "    }\n",
    "    # processing diff\n",
    "    print(f\"latitude_def : {data[1].latitude: >10} | latitude_xml : {dict_xml['latitude']: >10} | diff: {data[1].latitude_diff: >10}\")\n",
    "    print(f\"longitude_def: {data[1].longitude: >10} | longitude_xml: {dict_xml['longitude']: >10} | diff: {data[1].longitude_diff: >10}\")\n",
    "    \n",
    "    # update db\n",
    "    db_pool.execute(\n",
    "        sql,\n",
    "        (dict_xml['latitude'], dict_xml['longitude'], dict_xml['code']),\n",
    "        commit=True\n",
    "    )   \n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"process finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPDATE STATIONS_SENSOR METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update stations sensor data\n",
    "stations_db = pd.read_sql('select code,latitude,longitude from stations', con=engine2)\n",
    "\n",
    "# define unique columns for upsert\n",
    "table_name = 'stations_sensor'\n",
    "unique_cols = ['code','location','channel']\n",
    "\n",
    "# setup upset method\n",
    "def postgres_upsert_method(table, conn, keys, data_iter):\n",
    "    insert_stmt = postgresql.insert(table.table).values(list(data_iter))\n",
    "    on_conflict_stmt = insert_stmt.on_conflict_do_update(\n",
    "        index_elements=unique_cols,\n",
    "        set_={c.name: c for c in insert_stmt.excluded if c.name not in unique_cols}\n",
    "    )\n",
    "    conn.execute(on_conflict_stmt)\n",
    "\n",
    "sensor_df = pd.DataFrame(columns=['code','location','channel','sensor'])\n",
    "loop_obj = tqdm(stations_db['code'].tolist())\n",
    "for station in loop_obj:\n",
    "    try:\n",
    "        loop_obj.set_description(\"Processing %s\" % station)\n",
    "        # get data\n",
    "        url = f'http://202.90.198.40/sismon-wrs/web/detail_slmon2/{station}'\n",
    "        html_ = requests.get(url).content\n",
    "        df_list = pd.read_html(html_) # type: ignore\n",
    "        \n",
    "        # process data\n",
    "        temp_df = df_list[0].copy()\n",
    "        temp_df[\"Station/Channel\"] = temp_df[\"Station/Channel\"].str.split(\" \")\n",
    "        temp_df[\"channel\"] = temp_df[\"Station/Channel\"].apply(lambda x: x[1] if not x[1].isnumeric() else x[2])\n",
    "        temp_df[\"location\"] = temp_df[\"Station/Channel\"].apply(lambda x: x[1] if x[1].isnumeric() else '')\n",
    "        temp_df[\"sensor\"] = temp_df[\"Sensor Type\"]\n",
    "        temp_df[\"code\"] = temp_df[\"Station/Channel\"].apply(lambda x: x[0])\n",
    "        temp_df[\"Year\"] = temp_df[\"Sensor Type\"].apply(lambda x: x.split(\"-\")[-1])\n",
    "        temp_df = temp_df[[\"code\",\"location\",\"channel\",\"sensor\"]]\n",
    "\n",
    "        # concanate data\n",
    "        sensor_df = pd.concat([sensor_df,temp_df], ignore_index=True)\n",
    "\n",
    "        # remove unavailable sensor data\n",
    "        sensor_df = sensor_df[sensor_df.sensor != \"xxx\"]\n",
    "\n",
    "        # clear\n",
    "        del(temp_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing station {station}: {e}\")\n",
    "        # if error, continue to next station\n",
    "        continue\n",
    "\n",
    "# push to database\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"Pushing data to Database\")\n",
    "sensor_df.to_sql('stations_sensor', \n",
    "                 con=engine2, \n",
    "                 if_exists='append', \n",
    "                 index=False,\n",
    "                 method=postgres_upsert_method)\n",
    "\n",
    "# details\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"Details:\")\n",
    "non_colocated = 0\n",
    "non_colocated_list = []\n",
    "colocated = 0\n",
    "colocated_list = []\n",
    "other = 0\n",
    "other_list = []\n",
    "for sensor in stations_db.code:\n",
    "    tmp = sensor_df[sensor_df['code'] == sensor]\n",
    "    if len(tmp) == 3:\n",
    "        non_colocated+=1\n",
    "        non_colocated_list.append(sensor)\n",
    "    elif len(tmp) == 6:\n",
    "        colocated+=1\n",
    "        colocated_list.append(sensor)\n",
    "    else:\n",
    "        other+=1\n",
    "        other_list.append(sensor)\n",
    "\n",
    "print(\"non_colocated\",non_colocated)\n",
    "print(non_colocated_list)\n",
    "print(\"colocated\",colocated)\n",
    "print(colocated_list)\n",
    "print(\"other\",other)\n",
    "print(other_list)\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"process finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL UPDATE METADATA WITH EXTERNAL FILE(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_db = pd.read_sql('select * from stations', con=engine2)\n",
    "df = pd.read_csv(\"../../files/stasiun_seismik_prioritas_inatews_2025.csv\")\n",
    "print(\"stations_db columns:\", stations_db.columns)\n",
    "print(\"df columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "df_not_in_db = list(set(df['KODE'].tolist()) - set(stations_db['code'].tolist()))\n",
    "print(f\"Stations in df not in DB: {df_not_in_db}\")\n",
    "db_not_in_df = list(set(stations_db['code'].tolist()) - set(df['KODE'].tolist()))\n",
    "print(f\"Stations in db not in df: {db_not_in_df}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update metadata year and network_group\n",
    "for sta in stations_db.code:\n",
    "    if sta in df['KODE'].tolist():\n",
    "        year = int(df[df['KODE'] == sta]['TAHUN INSTALASI SITE'].values[0])\n",
    "        network_group = str(df[df['KODE'] == sta]['JARINGAN'].values[0])\n",
    "        print(f\"Updating {sta} with year: {year}, network_group: {network_group}\")\n",
    "\n",
    "        db_pool.execute(\n",
    "            \"UPDATE stations SET year=%s, network_group=%s WHERE code=%s\",\n",
    "            (year, network_group, sta),\n",
    "            commit=True\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Station {sta} not found in df, skipping update.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stations_db.upt.unique():\n",
    "    print(f\"upt: {i} -> balai: {stations_db[stations_db.upt == i].balai.unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Dominant Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_dates(start_date_iso, end_date_iso):\n",
    "    # Parse the ISO 8601 date inputs\n",
    "    start_date = datetime.fromisoformat(start_date_iso)\n",
    "    end_date = datetime.fromisoformat(end_date_iso)\n",
    "    \n",
    "    # Check if start date is before end date\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"Start date must be before end date\")\n",
    "    \n",
    "    # Create a list to store the dates\n",
    "    dates = []\n",
    "    \n",
    "    # Generate the list of dates in the ISO 8601 format\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        if current_date == end_date:\n",
    "            date_str = current_date.strftime(\"%Y-%m-%dT23:59:59\")\n",
    "        else:\n",
    "            date_str = current_date.strftime(\"%Y-%m-%dT00:00:00\")\n",
    "        dates.append(date_str)\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_data_quality =  pd.read_sql(f'select code,result,date from stations_data_quality where date >= \\'2025-01-01\\' and date <= \\'2025-12-31\\'', engine2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_value_dict = {}\n",
    "for code in stations_data_quality.code.unique():\n",
    "    value = stations_data_quality[stations_data_quality.code == code].value_counts(subset='result').index.values[0]\n",
    "    dominant_value_dict[code]=value\n",
    "df_dominant_quality = pd.DataFrame.from_dict(dominant_value_dict, orient='index',\n",
    "                    columns=['dominant_data_quality'])\n",
    "df_dominant_quality = df_dominant_quality.rename_axis('code').reset_index()\n",
    "# rename dominant data quality condition\n",
    "df_dominant_quality[\"dominant_data_quality\"] = df_dominant_quality[\"dominant_data_quality\"].replace({\n",
    "    'Mati': 'No Data',\n",
    "    'Buruk':'Poor',\n",
    "    'Cukup Baik':'Fair',\n",
    "    'Baik': 'Good'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"UPDATE stations_dominant_data_quality SET dominant_data_quality=%s WHERE code=%s\"\"\"\n",
    "for i,val in df_dominant_quality.iterrows():\n",
    "    print(val.code,val.dominant_data_quality)\n",
    "    with engine2.connect() as cur:\n",
    "        cur.execute(sql,(val.dominant_data_quality,val.code,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
